{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MATH692 - Numerical Optimization\n",
    "\n",
    "\n",
    "# Fundamentals of Unconstrained Optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The problem is \n",
    "\n",
    "$$\n",
    "\\min_{x\\in \\mathbb{R}^n} f(x)\n",
    "$$\n",
    "\n",
    "where $f: \\mathbb{R}^n\\to \\mathbb{R}$ is a real-valued smooth function defined on the Euclidean space $\\mathbb{R}^n$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What is a solution?\n",
    "\n",
    "#### Global Solution\n",
    "A point $x^*$ is a __global minimizer__ if $f(x^*) \\leq f(x)$ for all $x$\n",
    "\n",
    "#### Local Solution\n",
    "A point $x^*$ is a __weak (strong) local minimizer__ if there is a neighborhood $\\mathcal{N}$ of $x^*$  such that \n",
    "\n",
    "$$f(x^*) \\leq (<) f(x) \\quad \\text{for all }  x \\in \\mathcal{N} (\\setminus  \\{x^*\\})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Theorem (Taylor’s Theorem).\n",
    "Suppose that $f : \\mathbb{R}^n \\to \\mathbb{R}^n$ is continuously differentiable and that $p \\in \\mathbb{R}^n$. Then we have\n",
    "that\n",
    "$$ f (x + p) =  f (x) + \\nabla f (x + tp)^Tp, \\text{  for some  } t \\in (0, 1).$$ \n",
    "Moreover, if $f$ is twice continuously differentiable, we have that\n",
    "$$ \\nabla f(x + p) = \\nabla f(x) +\\int^1_0 \\nabla^2 f (x + tp) p dt, $$\n",
    "and that \n",
    "$$ f (x + p) = f (x) + \\nabla f (x)^T p + {1 \\over 2} p^T \\nabla^2 f(x + tp)p, \\text{ for some} t \\in (0, 1).$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Remark\n",
    "For convex functions every __local minimizer__ is also a __global minimizer__. (why?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Theorem (First-Order Necessary Conditions).\n",
    "If $x^∗$ is a local minimizer and $f$ is continuously differentiable in an open neighborhood of $x^*$, then $x^*$ is a __stationary point__, i.e.\n",
    "$$ \\nabla f(x^*) = 0.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Remark\n",
    "When $f$ is convex and  differentiable, then any stationary point $x^*$ is a global minimizer of $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Theorem (Second-Order Necessary Conditions).\n",
    "If $x^*$ is a local minimizer of $f$ and $\\nabla^2 f$ exists and is continuous in an open neighborhood\n",
    "of $x^*$, then \n",
    "$$ \\nabla f(x^*) = 0 \\text{ and }  \\nabla^2 f(x^*) \\text{ is positive semidefinite}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Theorem (Second-Order Sufficient Conditions).\n",
    "Suppose that $\\nabla^2 f$ is continuous in an open neighborhood of $x^*$ and that $\\nabla f (x^*) = 0$ and $\\nabla^2 f(x^*)$ is positive definite. Then $x^*$ is a strict local minimizer of $f$ .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convex sets and Convex functions\n",
    "\n",
    "#### Convex Sets:\n",
    "A set $S \\in \\mathbb{R}^n$ is a __convex set__ if the straight line segment connecting any two points in $S$ lies entirely inside $S$. i.e. for any two points $x \\in S$ and $y \\in S$, we have \n",
    "    $$ \\alpha x +(1-\\alpha)y \\in S \\quad \\text{for all}  \\; \\alpha \\in [0, 1].$$\n",
    "\n",
    "#### Convex Functions:\n",
    "The function $f$ is a __convex function__ if its domain $S$ is a convex set and if for any two points $x$ and $y$ in $S$, the following property is satisfied:\n",
    "$$ f (αx + (1 − α)y) ≤ αf (x) + (1 − α) f (y), \\text{  for all } α ∈ [0, 1].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Theorem. \n",
    "When $f$ is convex, any local minimizer $x^*$ is a global minimizer of $f$. If in addition $f$ is differentiable, then any stationary point $x^*$ is a global minimizer of $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Algorithms \n",
    "* For unconstrained optimization of smooth functions (chapters: 3,4,5,6,7).\n",
    "* Beginning at $x0$, __optimization algorithms__ generate a sequence of iterates $\\{x_k\\}$ that terminate when either no more progress can be made or when it seems that a solution point has been approximated with sufficient accuracy.\n",
    "* An algorithm finds a new iterate $x_{k+1}$ with a lower function value than $x_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Two strategies\n",
    "1. Line search methods (chapter 3)\n",
    "2. Trust Region methods (chapter 4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Line Search \n",
    "\n",
    "1. Given a point $x_k$, find a __descent direction__ $p_k$.\n",
    "2. Find the step length $\\alpha_k$ to minimize $f (x_k+\\alpha_k p_k)$. i.e. we solve \n",
    "$$\n",
    "\\min_{\\alpha >0} f(x_k+\\alpha p_k)\n",
    "$$\n",
    "3. Next point $x_{k+1}= x_k+ \\alpha_k p_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Trust Region \n",
    "1. For a function $f$, construct a __model function__ $m_k$ that approximates $f$ locally.\n",
    "2. Define a trust region $\\mathcal{R}(x_k)$ inside which $f \\approx m_k$.\n",
    "3. Solve the minimization problem: \n",
    "$$ \\min_p m_k(x_k+p) \\text{   where   } p \\text{  lies inside  } \\mathcal{R}(x_k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Methods to find descent directions for line search\n",
    "* #### Method 1: the steepest descent method $p_k=-\\nabla f(x_k)$\n",
    "* #### Method 2: the Newton's method\n",
    "* #### Method 3: the conjugate gradient method (chap 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Method 1: the steepest descent method $p_k=-\\nabla f(x_k)$\n",
    "- First order approximation\n",
    "- Linear convergence (global convergence)\n",
    "\n",
    "![steepest_descent_direction](./imgs/steepest_descent_direction.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Method 2: the Newton's method\n",
    "- Second order approximation\n",
    "- Quadratic convergence (local convergence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3: the conjugate gradient method (chap 5)\n",
    "\n",
    "The current search direction $p_k$ is a linear combination of previous search direction $p_{k-1}$ and\n",
    "current gradient \n",
    "$$ p_k = −∇f(x_k) + β_kp_{k−1} $$\n",
    "\n",
    "- Scalar βk is given such that $p_k$ and $p_{k-1}$ are conjugate (definition later).\n"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.4.2",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
